{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caf1f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the suitable libraries\n",
    "\n",
    "import os\n",
    "import torch \n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "\n",
    "from utils import get_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a68130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the constants. \n",
    "#A constant is a particular kind of variable that stores values that are fixed.\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "NOISE_FACTOR = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4135463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data\n",
    "\n",
    "transform = get_transform()\n",
    "#Converting the picture pixel data to PyTorch tensors and normalising the values. \n",
    "#The pixel values will fall inside the range [0, 1] after being normalised. \n",
    "#Compared to the [0, 256] pixel value range, Normalize function leads to quicker training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236c55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the MNIST_Fashion Dataset.\n",
    "#Getting ready the trainloader, testloader and validation sets for the respective training and testing.\n",
    "\n",
    "trainset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "testset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "valset = datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "#Splitting the dataset\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    testset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2618e884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa54add19a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Getting a batch of training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "#Visualising a random image from the dataset.\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f144d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (enc1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (enc2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (enc3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (enc4): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dec1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (dec2): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
      "  (dec3): ConvTranspose2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (dec4): ConvTranspose2d(32, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (out): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#building the autoencoder network with encoder and decoder\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # encoder layers\n",
    "        self.enc1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.enc2 = nn.Conv2d(64, 32, kernel_size=3, padding=1)\n",
    "        self.enc3 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.enc4 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # decoder layers\n",
    "        self.dec1 = nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2)  \n",
    "        self.dec2 = nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2)\n",
    "        self.dec3 = nn.ConvTranspose2d(16, 32, kernel_size=2, stride=2)\n",
    "        self.dec4 = nn.ConvTranspose2d(32, 64, kernel_size=2, stride=2)\n",
    "        self.out = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = self.pool(x) # the latent space representation\n",
    "        \n",
    "        # decode\n",
    "        x = F.relu(self.dec1(x))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        x = F.sigmoid(self.out(x))\n",
    "        return x\n",
    "net = Autoencoder()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17674342",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2871bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8318ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "643b541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "def train(net, trainloader, NUM_EPOCHS, writer):\n",
    "#     train_loss = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        net.train()\n",
    "        for data in trainloader:\n",
    "            img, _ = data \n",
    "            # add noise to the image data\n",
    "            img_noisy = img + NOISE_FACTOR * torch.randn(img.shape)\n",
    "            # clip to make the values fall between 0 and 1\n",
    "            img_noisy = np.clip(img_noisy, 0., 1.)\n",
    "            img_noisy = img_noisy.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(img_noisy)\n",
    "            loss = criterion(outputs, img_noisy)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            # updating the parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "#         train_loss.append(loss)\n",
    "        \n",
    "        net.eval()\n",
    "        running_loss_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for img, _ in val_loader:\n",
    "                # add noise to the image data\n",
    "                img_noisy = img + NOISE_FACTOR * torch.randn(img.shape)\n",
    "                # clip to make the values fall between 0 and 1\n",
    "                img_noisy = np.clip(img_noisy, 0., 1.)\n",
    "                img_noisy = img_noisy.to(device)\n",
    "                output = net(img_noisy)\n",
    "                loss = criterion(output, img_noisy)\n",
    "                running_loss_val += loss.item()\n",
    "\n",
    "        loss_epoch = running_loss_val / len(val_loader) \n",
    "        writer.add_scalar(\"Loss/val\", loss_epoch, epoch)\n",
    "        print('Epoch {} of {}, Train Loss: {:.3f}, Val Loss: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, loss, loss_epoch))\n",
    "        save_decoded_image(img_noisy.cpu().data, name='./Saved_Images/noisy{}.png'.format(epoch))\n",
    "        save_decoded_image(outputs.cpu().data, name='./Saved_Images/denoised{}.png'.format(epoch))\n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c75765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation function\n",
    "\n",
    "# def validate(net, val_loader, writer):\n",
    "#     model.eval()\n",
    "#     total_loss = []\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in val_loader:\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             output = model(images)\n",
    "#             loss = criterion\n",
    "#             total_loss += loss.item()\n",
    "#             pred = output.argmax(dim=1)\n",
    "    \n",
    "#     loss_epoch = total_loss / len(val_loader) / len(val_loader.dataset) \n",
    "#     writer.add_scalar(\"Loss/val\", loss_epoch)\n",
    "#     return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf6cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image_reconstruction(net, testloader):\n",
    "     for batch in testloader:\n",
    "        img, _ = batch\n",
    "        img_noisy = img + NOISE_FACTOR * torch.randn(img.shape)\n",
    "        img_noisy = np.clip(img_noisy, 0., 1.)\n",
    "        img_noisy = img_noisy.to(device)\n",
    "        outputs = net(img_noisy)\n",
    "        outputs = outputs.view(outputs.size(0), 1, 28, 28).cpu().data\n",
    "        save_image(img_noisy, 'noisy_test_input.png')\n",
    "        save_image(outputs, 'denoised_test_reconstruction.png')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "277d01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "def make_dir():\n",
    "    image_dir = 'Saved_Images'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "def save_decoded_image(img, name):\n",
    "    img = img.view(img.size(0), 1, 28, 28)\n",
    "    save_image(img, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58fb7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s5532166/.pyenv/versions/anaconda3-2022.05/envs/ML_Labs/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 20, Train Loss: 0.058, Val Loss: 0.058\n",
      "Epoch 2 of 20, Train Loss: 0.064, Val Loss: 0.057\n",
      "Epoch 3 of 20, Train Loss: 0.071, Val Loss: 0.056\n",
      "Epoch 4 of 20, Train Loss: 0.042, Val Loss: 0.056\n",
      "Epoch 5 of 20, Train Loss: 0.058, Val Loss: 0.056\n",
      "Epoch 6 of 20, Train Loss: 0.057, Val Loss: 0.055\n",
      "Epoch 7 of 20, Train Loss: 0.058, Val Loss: 0.055\n",
      "Epoch 8 of 20, Train Loss: 0.048, Val Loss: 0.055\n",
      "Epoch 9 of 20, Train Loss: 0.052, Val Loss: 0.055\n",
      "Epoch 10 of 20, Train Loss: 0.057, Val Loss: 0.055\n",
      "Epoch 11 of 20, Train Loss: 0.058, Val Loss: 0.055\n",
      "Epoch 12 of 20, Train Loss: 0.061, Val Loss: 0.055\n",
      "Epoch 13 of 20, Train Loss: 0.055, Val Loss: 0.055\n",
      "Epoch 14 of 20, Train Loss: 0.048, Val Loss: 0.055\n",
      "Epoch 15 of 20, Train Loss: 0.057, Val Loss: 0.055\n",
      "Epoch 16 of 20, Train Loss: 0.065, Val Loss: 0.055\n",
      "Epoch 17 of 20, Train Loss: 0.055, Val Loss: 0.054\n",
      "Epoch 18 of 20, Train Loss: 0.053, Val Loss: 0.055\n",
      "Epoch 19 of 20, Train Loss: 0.067, Val Loss: 0.054\n",
      "Epoch 20 of 20, Train Loss: 0.048, Val Loss: 0.054\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m make_dir()\n\u001b[0;32m----> 5\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# plt.figure()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# plt.plot(train_loss)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# plt.title('Traidef validate(net, val_loader, writer):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# plt.ylabel('Loss')\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# plt.savefig('./Saved_Images/conv_ae_fahsionmnist_loss.png')\u001b[39;00m\n\u001b[1;32m     26\u001b[0m test_image_reconstruction(net, testloader)\n",
      "Cell \u001b[0;32mIn [12], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, trainloader, NUM_EPOCHS, writer)\u001b[0m\n\u001b[1;32m     44\u001b[0m     save_decoded_image(img_noisy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Saved_Images/noisy\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[1;32m     45\u001b[0m     save_decoded_image(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Saved_Images/denoised\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_loss\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "print(device)\n",
    "net.to(device)\n",
    "make_dir()\n",
    "train_loss = train(net, trainloader, NUM_EPOCHS, tb_writer)\n",
    "# plt.figure()\n",
    "# plt.plot(train_loss)\n",
    "# plt.title('Traidef validate(net, val_loader, writer):\n",
    "# #     model.eval()\n",
    "# #     total_loss = []\n",
    "# #     with torch.no_grad():\n",
    "# #         for images, labels in val_loader:\n",
    "# #             images = images.to(device)\n",
    "# #             labels = labels.to(device)\n",
    "# #             output = model(images)\n",
    "# #             loss = criterion\n",
    "# #             total_loss += loss.item()\n",
    "# #             pred = output.argmax(dim=1)\n",
    "    \n",
    "# #     loss_epoch = total_loss / len(val_loader) / len(val_loader.dataset) \n",
    "# #     writer.add_scalar(\"Loss/val\", loss_epoch)\n",
    "# #     return loss_epochn Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.savefig('./Saved_Images/conv_ae_fahsionmnist_loss.png')\n",
    "test_image_reconstruction(net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473dee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising noisy image saved during the training.\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "fig = plt.figure(figsize = (15,15)) \n",
    "\n",
    "img = mpimg.imread(\"noisy_test_input.png\")\n",
    "plt.title('noisy input')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e45201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the denoised image \n",
    "\n",
    "fig = plt.figure(figsize = (15,15)) \n",
    "\n",
    "img = mpimg.imread(\"denoised_test_reconstruction.png\")\n",
    "plt.title('denoised output')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating checkpoints\n",
    "if (NUM_EPOCHS+1):\n",
    "    checkpoint_path = \"checkpoint_{}.pth\".format(NUM_EPOCHS+1)\n",
    "    torch.save({'NUM_EPOCHS': NUM_EPOCHS+0,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()},\n",
    "                   checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9d0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deployment(loading the model from the checkpoint)\n",
    "\n",
    "checkpoint_path = \"checkpoint_21.pth\"\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "NUM_EPOCHS = checkpoint['NUM_EPOCHS']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
